optimizer: 'adamw'

lr: 1.0e-5
# learning rate is scaled to lr * batch_size / 256
# drop learning rate even more to prevent embedding loss from blowing up
weight_decay: 0.01
momentum: 0.9
batch_size: 128

epochs: 100
start_epoch: 0

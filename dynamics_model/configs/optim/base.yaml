optimizer: 'adamw'

lr: 1.5e-4
# learning rate is scaled to lr * batch_size / 256
weight_decay: 0.1
momentum: 0.9
batch_size: 256

epochs: 300
warmup_epochs: 40
start_finetune: 100
start_epoch: 0

refresh_optimizer: True
refresh_lr: 1.0e-5

schedule: [120, 160]
cos: True

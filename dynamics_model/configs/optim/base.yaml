optimizer: 'adamw'

lr: 1.0e-4
# learning rate is scaled to lr * batch_size / 256
weight_decay: 0.01
momentum: 0.9
batch_size: 128

epochs: 100
start_epoch: 0
